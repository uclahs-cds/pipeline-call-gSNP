import nextflow.util.SysHelper
includeConfig "${projectDir}/external/nextflow-config/config/csv/csv_parser.config"
includeConfig "${projectDir}/external/nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/nextflow-config/config/methods/common_methods.config"

class log_output_dir {
  static def check_permissions(path) {
    def filePath = new File(path)

    if (filePath.exists()) {
      if (filePath.canWrite()) {
        return
      }
      throw new Exception("${path} is not writable")
    }

    // Attempts to create directory if the path does not exist
    if (!filePath.mkdirs()) {
      throw new Exception("${path} does not exist and could not create")
    }
  }
}

methods {

  // Function to ensure that resource requirements don't go beyond
  // a maximum limit
  check_max = { obj, type ->
    if (type == 'memory') {
      try {
        if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
          return params.max_memory as nextflow.util.MemoryUnit
        else
          return obj
      } catch (all) {
        println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
        return obj
      }
    } else if (type == 'time') {
      try {
        if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
          return params.max_time as nextflow.util.Duration
        else
          return obj
      } catch (all) {
        println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
        return obj
      }
    } else if (type == 'cpus') {
      try {
        return Math.min(obj, params.max_cpus as int)
      } catch (all) {
        println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
        return obj
      }
    }
  }

  set_allocation = {
    // Set base and node-specific allocations
    methods.set_resources_allocation()

    // Set the memory for BaseRecalibrator based on number of samples being processed
    def node_cpus = SysHelper.getAvailCpus()
    def node_memory_GB = SysHelper.getAvailMemory().toGiga()

    if (params.containsKey('input')) {
      num_sample_lines = params.input.BAM.tumour.size()
    } else {
      num_sample_lines = new File(params.input_csv).readLines().size() - 1
    }

    // Each line contains a normal and tumour. Normal sample is common so total num of samples: tumour samples + 1 normal sample
    num_samples = num_sample_lines + 1

    // Single sample -> allocate 75% of total memory
    // Multi sample -> divide memory per sample, with a min of 20% of total memory
    mem_to_allocate = (params.is_NT_paired) ? \
      Math.max(node_memory_GB * 0.2, (node_memory_GB * 0.9) / num_samples) : \
      node_memory_GB * 0.75 
    
    process['withName:run_BaseRecalibrator_GATK'].memory = "${mem_to_allocate} GB"
  }

  set_ids_from_csv = { List parsed_csv ->
    def patient_ids = [] as Set
    def sample_ids = [] as Set

    parsed_csv.each { csv_line -> [patient_ids.add(csv_line['patient_id']), sample_ids.add(csv_line['sample_id'])] }

    if (patient_ids.size() != 1) {
        throw new Exception("Multiple patient IDs detected: ${patient_ids}. Please submit only a single patient per job.")
    }

    if (sample_ids.size() != 1) {
        throw new Exception("Multiple sample IDs detected: ${sample_ids}. Please submit normal and/or tumour BAMs for one sample per job.")
    }

    params.patient_id = patient_ids[0]
    params.sample_id = sample_ids[0]
  }

  set_log_output_dir = {
    def tz = TimeZone.getTimeZone("UTC")
    def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)
              
    params.avere_prefix = '/hot/data'

    if (params.blcds_registered_dataset) {
      if ("${params.dataset_id.length()}" != 11) {
        throw new Exception("Dataset id must be eleven characters long")
      }
      def disease = "${params.dataset_id.substring(0,4)}"
      // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
      params.log_output_dir = "${params.avere_prefix}/$disease/${params.dataset_id}/${params.patient_id}/${params.sample_id}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/${date}"
      params.disease = "${disease}"
    } else {
      params.log_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample_id}/log-${manifest.name}-${manifest.version}-${date}"
      params.disease = null
    }

    params.date = "${date}"
  }

  set_output_dir = {
    params.output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample_id}/${params.docker_image_gatk.split("/")[1].replace(':', '-').toUpperCase()}"
  }

  check_workdir_permissions = { dir ->
    dir_file = new File(dir)
    if (dir_file.exists()) {
      if (dir_file.canWrite()) {
        return true
      } else {
        throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} is not writeable. Please verify and try again.")
      }
    } else {
      while (!dir_file.exists()) {
        dir_file = dir_file.getParentFile()
      }

      if (dir_file.canWrite()) {
        return true
      } else {
        throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} cannot be created. The closest existing parent directory ${dir_file.toString()} is not writable. Please verify permissions or change the input parameter.")
      }
    }
  }

  set_env = {
    if (params.ucla_cds) {
      /**
        * By default, if the /scratch directory exists, set it as the Nextflow working directory
        * If config file specified work_dir, set it as the Nextflow working directory
        * 
        * WARNING: changing this directory can lead to high server latency and
        * potential disk space limitations. Change with caution! The 'workDir'
        * in Nextflow determines the location of intermediate and temporary files.
        */
      params.work_dir = (params.containsKey("work_dir") && params.work_dir) ? params.work_dir : "/scratch"
      if (methods.check_workdir_permissions(params.work_dir)) {
          workDir = params.work_dir
      }
    } else {
      // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
      if (params.containsKey("work_dir") && params.work_dir && methods.check_workdir_permissions(params.work_dir)) {
        workDir = params.work_dir
      } else {
          params.work_dir = "${launchDir}/work"
      }
    }
  }

  // Pipeline monitoring and metric files
  set_timeline = {
    timeline.enabled = true
    timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"
  }

  set_trace = {
    trace.enabled = true
    trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"
  }

  set_report = {
    report.enabled = true
    report.file = "${params.log_output_dir}/nextflow-log/report.html"
  }

  detect_mode = {
    // Detect whether the job is for a targeted sample
    params.is_targeted = (params.intervals) ? true : false
  }

  check_hc_output_mode = {
    if ( params.emit_all_confident_sites && !params.is_targeted ) {
        System.out.println("### WARNING ### Emitting all confident sites in whole genome mode! Be careful with getting large numbers of reference calls!")
    }
  }

  format_input_from_yaml = {
    if (! params.is_NT_paired) {
      // Move single sample to normal by default regardless of type
      params.input.BAM.normal = params.input.BAM[params.single_sample_type]
      params.input.BAM.tumour = [[id: "NA", path: "${params.work_dir}/NO_FILE.bam"]]
    }
    if (params.input.BAM.normal.size() != 1) {
      throw new Exception("Multiple normal samples detected: ${params.input.BAM.normal}! Only a single normal sample is allowed.")
    }
    // Mix the normal sample with all tumour samples
    def normal_bam_id = params.input.BAM.normal[0]['id']
    def normal_bam = params.input.BAM.normal[0]['path']
    for (i in params.input.BAM.tumour) {
      i.tumour_id = i['id']
      i.tumour_BAM = i['path']
      i.normal_id = normal_bam_id
      i.normal_BAM = normal_bam
      i.sample_id = params.sample_id
    }
  }

  extract_bam_and_id = { List parsed_csv, String type ->
    def extracted_inputs = [] as Set
    parsed_csv.each { csv_line ->
      extracted_inputs.add(['id': csv_line["${type}_id"], 'path': csv_line["${type}_BAM"]])
    }
    return extracted_inputs
  }

  format_csv_input = { List parsed_csv ->
    params.input = ['BAM': [:]]
    if (params.is_NT_paired) {
      params.input.BAM['normal'] = methods.extract_bam_and_id(parsed_csv, 'normal')
      params.input.BAM['tumour'] = methods.extract_bam_and_id(parsed_csv, 'tumour')
    } else {
      params.input.BAM[params.single_sample_type] = methods.extract_bam_and_id(parsed_csv, params.single_sample_type)
    }
  }

  convert_to_yaml_input = {
    if (params.containsKey('input')) {
      // YAML was used so set modes accordingly
      params.is_NT_paired = params.input.BAM.containsKey('normal') && params.input.BAM.containsKey('tumour')
      params.single_sample_type = null
      if (!params.is_NT_paired) {
        all_input_keys = params.input.BAM.keySet() as String[]
        if (all_input_keys.size() != 1) {
          throw new Exception("Multiple types of BAMs found despite being a single sample run! Check input YAML.")
        }
        params.single_sample_type = all_input_keys[0]
      }
    } else if (params.containsKey('input_csv')) {
      // Parse CSV header line and determine modes
      def reader = new BufferedReader(new FileReader(params.input_csv))
      header_line = reader.readLine()
      params.is_NT_paired = header_line.contains('normal_BAM') && header_line.contains('tumour_BAM')
      params.single_sample_type = null
      if (!params.is_NT_paired) {
        if (header_line.contains('normal_BAM')) {
          params.single_sample_type = 'normal'
        } else if (header_line.contains('tumour_BAM')) {
          params.single_sample_type = 'tumour'
        } else {
          throw new Exception("Input CSV does not contain proper BAM types! Allowed: normal_BAM, tumour_BAM")
        }
      }
      def csv_header_fields = header_line.split(',') as List
      def raw_csv_input = csv_parser.parse_csv(params.input_csv, csv_header_fields)
      methods.set_ids_from_csv(raw_csv_input)
      // Format the CSV input to match input YAML format
      methods.format_csv_input(raw_csv_input)
    } else {
      throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
    }
  }

  set_cache = {
      process.cache = params.cache_intermediate_pipeline_steps
  }

  // Set up env, timeline, trace, and report above.
  setup = {
    methods.set_env()
    methods.set_cache()
    methods.convert_to_yaml_input()
    schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
    schema.validate()
    methods.format_input_from_yaml()
    methods.set_log_output_dir()
    methods.set_output_dir()
    log_output_dir.check_permissions(params.log_output_dir)
    methods.set_allocation()
    methods.set_timeline()
    methods.set_trace()
    methods.set_report()
    methods.detect_mode()
    methods.check_hc_output_mode()
  }
}
