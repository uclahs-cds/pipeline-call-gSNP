import nextflow.util.SysHelper

class log_output_dir {
  static def check_permissions(path) {
    def filePath = new File(path)

    if (filePath.exists()) {
      if (filePath.canWrite()) {
        return
      }
      throw new Exception("${path} is not writable")
    }

    // Attempts to create directory if the path does not exist
    if (!filePath.mkdirs()) {
      throw new Exception("${path} does not exist and could not create")
    }
  }
}

methods {

  // Function to ensure that resource requirements don't go beyond
  // a maximum limit
  check_max = { obj, type ->
    if (type == 'memory') {
      try {
        if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
          return params.max_memory as nextflow.util.MemoryUnit
        else
          return obj
      } catch (all) {
        println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
        return obj
      }
    } else if (type == 'time') {
      try {
        if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
          return params.max_time as nextflow.util.Duration
        else
          return obj
      } catch (all) {
        println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
        return obj
      }
    } else if (type == 'cpus') {
      try {
        return Math.min(obj, params.max_cpus as int)
      } catch (all) {
        println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
        return obj
      }
    }
  }

  set_resources_allocation = {
    def node_cpus = SysHelper.getAvailCpus()
    def node_memory_GB = SysHelper.getAvailMemory().toGiga()
    // Load base.config by default for all pipelines
    includeConfig "${projectDir}/config/base.config"
    if (params.ucla_cds) {
      if (node_cpus == 64) {
        // Check memory for M64 node
        if (node_memory_GB == 64 && node_memory_GB >= 950 && node_memory_GB <= 1010) {
          includeConfig "${projectDir}/config/M64.config"
        } else {
          throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
        }
      } else {
        // Check memory for F series node
        if (node_memory_GB >= (node_cpus * 2 * 0.9) && node_memory_GB <= (node_cpus * 2)) {
          includeConfig "${projectDir}/config/F${node_cpus}.config"
        } else {
          throw new Exception("   ### ERROR ###   System resources not as expected (cpus=${node_cpus} memory=${node_memory_GB}), unable to assign resources.")
        }
      }
    }

    // Set the memory for BaseRecalibrator based on number of samples being processed
    num_sample_lines = new File(params.input_csv).readLines().size() - 1

    // Each line contains a normal and tumour. Normal sample is common so total num of samples: tumour samples + 1 normal sample
    num_samples = num_sample_lines + 1

    // Single sample -> allocate 75% of total memory
    // Multi sample -> divide memory per sample, with a min of 20% of total memory
    mem_to_allocate = (params.is_NT_paired) ? \
      Math.max(node_memory_GB * 0.2, (node_memory_GB * 0.9) / num_samples) : \
      node_memory_GB * 0.75 
    
    process['withName:run_BaseRecalibrator_GATK'].memory = "${mem_to_allocate} GB"
  }

  set_ids_from_csv = {
    def project
    def sample

    def reader = new FileReader(params.input_csv)
    reader.splitEachLine(',') { parts -> [project = parts[0], sample = parts[1]] }

    params.project_id = "${project}"
    params.sample_id = "${sample}"
  }

  set_log_output_dir = {
    def tz = TimeZone.getTimeZone("UTC")
    def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)
              
    params.avere_prefix = '/hot/data'

    if (params.blcds_registered_dataset) {
      if ("${params.dataset_id.length()}" != 11) {
        throw new Exception("Dataset id must be eleven characters long")
      }
      def disease = "${params.dataset_id.substring(0,4)}"
      // Need to fill in analyte, technology, raw_od_aligned, genome, pipeline-name
      params.log_output_dir = "${params.avere_prefix}/$disease/${params.dataset_id}/${params.project_id}/${params.sample_id}/analyte/technology,raw_or_aligned/genome/logs/pipeline-name/${date}"
      params.disease = "${disease}"
    } else {
      params.log_output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample_id}/log-${manifest.name}-${manifest.version}-${date}"
      params.disease = null
    }

    params.date = "${date}"
  }

  set_output_dir = {
    params.output_dir = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample}/${params.docker_image_gatk.split("/")[1].replace(':', '-').toUpperCase()}"
  }

  check_workdir_permissions = { dir ->
    dir_file = new File(dir)
    if (dir_file.exists()) {
      if (dir_file.canWrite()) {
        return true
      } else {
        throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} is not writeable. Please verify and try again.")
      }
    } else {
      while (!dir_file.exists()) {
        dir_file = dir_file.getParentFile()
      }

      if (dir_file.canWrite()) {
        return true
      } else {
        throw new Exception("   ### ERROR ###   The input directory params.work_dir: ${dir} cannot be created. The closest existing parent directory ${dir_file.toString()} is not writable. Please verify permissions or change the input parameter.")
      }
    }
  }

  set_env = {
    if (params.ucla_cds) {
      /**
        * By default, if the /scratch directory exists, set it as the Nextflow working directory
        * If config file specified work_dir, set it as the Nextflow working directory
        * 
        * WARNING: changing this directory can lead to high server latency and
        * potential disk space limitations. Change with caution! The 'workDir'
        * in Nextflow determines the location of intermediate and temporary files.
        */
      params.work_dir = (params.containsKey("work_dir") && params.work_dir) ? params.work_dir : "/scratch"
      if (methods.check_workdir_permissions(params.work_dir)) {
          workDir = params.work_dir
      }
    } else {
      // If work_dir was specified as a param and exists or can be created, set workDir. Otherwise, let Nextflow's default behavior dictate workDir
      if (params.containsKey("work_dir") && params.work_dir && methods.check_workdir_permissions(params.work_dir)) {
        workDir = params.work_dir
      }
    }
  }

  // Pipeline monitoring and metric files
  set_timeline = {
    timeline.enabled = true
    timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"
  }

  set_trace = {
    trace.enabled = true
    trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"
  }

  set_report = {
    report.enabled = true
    report.file = "${params.log_output_dir}/nextflow-log/report.html"
  }

  detect_mode = {
    // Detect whether the job is for a targeted sample
    params.is_targeted = (params.intervals) ? true : false
  }

  format_input_from_yaml = {
    if ! params.is_NT_paired {
      // Move single sample to normal by default regardless of type
      params.input.BAM.normal = params.input.BAM[params.single_sample_type]
      params.input.BAM.tumour = [id: "NA", BAM: "/NO_PATH/NO_FILE.bam"]
    }
    if (len(params.input.BAM.normal) != 1) {
      throw new Exception("Multiple normal samples detected! Only a single normal sample is allowed.")
    }
    // Mix the normal sample with all tumour samples
    def normal_bam_id = params.input.BAM.normal[0].id
    def normal_bam = params.input.BAM.normal[0].BAM
    for (i in params.input.BAM.tumour) {
      i.tumour_id = i.id
      i.tumour_BAM = i.BAM
      i.normal_id = normal_bam_id
      i.normal_BAM = normal_bam
      i.sample_id = params.sample_id
    }
  }

  parse_input = {
    if params.containsKey('input') {
      // YAML was used so set modes accordingly
      params.is_NT_paired = params.input.containsKey('normal') && params.input.containsKey('tumor')
      params.single_sample_type = null
      if ! params.is_NT_paired {
        all_input_keys = params.input.keySet() as String[]
        if len(all_input_keys) != 1 {
          throw new Exception("Multiple types of BAMs found despite being a single sample run! Check input YAML.")
        }
        params.single_sample_type = all_input_keys[0]
      }
      params.input_mode = 'yaml'
    } else if params.containsKey('input_csv') {
      // Parse CSV header line and determine modes
      def reader = new BufferedReader(new FileReader(params.input_csv))
      header_line = reader.readLine()
      params.is_NT_paired = header_line.contains('normal_BAM') && header_line.contains('tumour_BAM')
      params.single_sample_type = null
      if ! params.is_NT_paired {
        if header_line.contains('normal_BAM') {
          params.single_sample_type = 'normal'
        } else if header_line.contains('tumour_BAM') {
          params.single_sample_type = 'tumour'
        } else {
          throw new Exception("Input CSV does not contain proper BAM types! Allowed: normal_BAM, tumour_BAM")
        }
      }
      methods.set_ids_from_csv()
      params.input_mode = 'csv'
    } else {
      throw new Exception("Neither YAML nor CSV inputs found! Please run pipeline with inputs.")
    }
  }

  // Set up env, timeline, trace, and report above.
  setup = {
    methods.parse_input()
    methods.set_log_output_dir()
    methods.set_output_dir()
    log_output_dir.check_permissions(params.log_output_dir)
    methods.set_resources_allocation()
    methods.set_env()
    methods.set_timeline()
    methods.set_trace()
    methods.set_report()
    methods.detect_mode()
  }
}
